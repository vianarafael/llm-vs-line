{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e87a55-4dba-4976-9dae-8ea2e06fc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d63bf1-66f3-4e29-a63f-653288db9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"qwen3:14b-q4_K_M\" # running in ollama on my local (ryzen 5 rx 3060 12GB)\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce5777b-199a-467c-b0f9-112b6fe09b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb7a04b-7fa0-4b65-8c6e-9dc9bbd80e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take everything in all the sub-folders of our knowledgebase \n",
    "# those are the result of scraping line campus material (scrape_shinsen.py and scrape_manabu.py)\n",
    "# since you need to be logged in to access the material I saved the cookies (just ran save_cookies.py and logged in)\n",
    "\n",
    "folders = glob.glob(\"knowledge_base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5460f7a-1d07-4f91-b3f2-5e1c1e5317c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1109 chunks.\n",
      "Vectorstore ready with 1109 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Split raw docs into small, overlapping chunks (~300 tokens each)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,         # roughly 300‑350 tokens\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "chunks = splitter.split_documents(documents)   \n",
    "\n",
    "print(f\"Prepared {len(chunks)} chunks.\")\n",
    "\n",
    "# 2. Create an embedding object that will respect OpenAI limits\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # very fast, good quality\n",
    "    chunk_size=200                   # ≤200 chunks per API call → ≤~60k tokens\n",
    ")\n",
    "\n",
    "# 3. (Re)build the vector DB\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_name,\n",
    ")\n",
    "print(f\"Vectorstore ready with {vectorstore._collection.count()} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eccebee8-744a-4803-9e73-226fd423d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from few_shots import SYSTEM_PROMPT, FEW_SHOTS # added some questions with the answer I knew, and a brief explanation\n",
    "\n",
    "messages = [(\"system\", SYSTEM_PROMPT)]\n",
    "for q, a in FEW_SHOTS:\n",
    "    messages.append((\"human\", q))\n",
    "    messages.append((\"assistant\", a))\n",
    "\n",
    "# finally the placeholders for retrieval context + new question\n",
    "messages.append(\n",
    "    (\"human\",\n",
    "     \"【文脈資料】\\n{context}\\n\\n\"\n",
    "     \"【質問】\\n{question}\\n\\n\"\n",
    "     \"まず簡単に思考ステップを列挙し、そのあと回答を出してください。\")\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add3a48f-56b5-42af-95a5-591b047ab043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. LLM — keep randomness low for exam‑style accuracy\n",
    "llm =  OllamaLLM(\n",
    "    model=MODEL,   \n",
    "    temperature=0.0,     # 0–0.3 = deterministic & focused\n",
    ")\n",
    "\n",
    "# 2. Retriever — wraps your Chroma vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6}) # Good balance: enough context to answer; still well under token limits\n",
    "\n",
    "# 3. Put it together\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm       = llm,\n",
    "    retriever = retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba4bfe2-e43d-4954-888e-ffba7b55b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この質問には答えが明確なものではないため、質問を分析してみましょう。\n",
      "\n",
      "1.  **LINE公式アカウントのUIDとは何ですか？** この質問は、LINE公式アカウントのユーザーID（uid）について尋ねているようです。ただし、この質問には明確な答えが得られません。\n",
      "2.  **LINE公式アカウントの UIDを知っていても、そのUIDが無効な場合はメッセージを送信できません。** この文は、ユーザーIDが有効かどうかを確認する方法について説明しています。しかし、この質問には明確な答えが得られません。\n",
      "3.  **プロフィール情報を取得するエンドポイントを使用して、ユーザーIDが有効かを確認できます。** この文は、プロフィール情報を取得するエンドポイントを使用して、ユーザーIDが有効かどうかを確認する方法について説明しています。しかし、この質問には明確な答えが得られません。\n",
      "\n",
      "この質問には明確な答えが得られないため、回答はありません。ただし、質問の内容は、LINE公式アカウントのユーザーID（uid）に関する情報を提供することに関連しています。\n"
     ]
    }
   ],
   "source": [
    "# Just testing a basic question\n",
    "question = \"LINE公式アカウントのUIDとは何ですか？\"\n",
    "\n",
    "response = qa_chain(               # or qa_chain.invoke(...) – same thing\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": [],       \n",
    "    }\n",
    ")\n",
    "\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd5d6e9-10cb-4996-b3b7-2a194fe54ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UI to paste the questions in\n",
    "import gradio as gr\n",
    "\n",
    "def answer_question(user_input):\n",
    "    if not user_input.strip():\n",
    "        return \"⚠️ 質問を入力してください\"\n",
    "    result = qa_chain.invoke({\n",
    "        \"question\": user_input,\n",
    "        \"chat_history\": []\n",
    "    })\n",
    "    return result[\"answer\"]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## LINE公式アカウント Advanced 試験アシスタント\")\n",
    "    inp = gr.Textbox(label=\"問題文と選択肢を貼り付けてください\",\n",
    "                     placeholder=\"問題文と4つの選択肢をここに入力\", lines=8)\n",
    "    out = gr.Textbox(label=\"回答\", lines=6, interactive=False)\n",
    "    btn = gr.Button(\"解答する\")\n",
    "    btn.click(answer_question, inputs=inp, outputs=out)\\\n",
    "       .then(lambda : \"\", None, inp)   # clear box after answer\n",
    "    demo.queue()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
